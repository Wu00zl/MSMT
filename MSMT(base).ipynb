{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed80b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from src.data_utils import *\n",
    "from src.eval_utils import *\n",
    "from src.TSCR import Span_Evalution\n",
    "\n",
    "from src.t5 import MyT5ForConditionalGeneration\n",
    "\n",
    "from transformers import  AutoModelForSeq2SeqLM,AutoTokenizer, AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffccb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def init_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # basic settings\n",
    "    parser.add_argument(\"--model_name_or_path\", default='/home/dell/peft/mt0-base', type=str,\n",
    "                        help=\"Path to pre-trained model or shortcut name\")\n",
    "    parser.add_argument(\"--train_dataset\", default='/home/dell/peft/seq2seq/datasets/JD_new_train.json', type=str)\n",
    "    parser.add_argument(\"--meituan_train_dataset\", default='/home/dell/peft/seq2seq/datasets/meituan_train_dataset.json', type=str)\n",
    "    parser.add_argument(\"--test_dataset\", default='/home/dell/peft/seq2seq/datasets/JD_new_test.json', type=str)\n",
    "    parser.add_argument(\"--input_max_len\", default=160, type=int)\n",
    "    parser.add_argument(\"--target_max_len\", default=350, type=int)\n",
    "\n",
    "    # other parameters\n",
    "    parser.add_argument(\"--max_seq_length\", default=350, type=int)\n",
    "    parser.add_argument(\"--train_batch_size\", default=5, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\"--eval_batch_size\", default=10, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-4, type=float)\n",
    "    parser.add_argument(\"--num_train_epochs\", default=100, type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "\n",
    "    # training details\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float)\n",
    "    parser.add_argument(\"--warmup_steps\", default=0.0, type=float)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # set up output dir which looks like './outputs/rest15/'\n",
    "    if not os.path.exists('./outputs'):\n",
    "        os.mkdir('./outputs')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "args = init_args()\n",
    "args.output_dir = './outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(args, model, dataloader, tokenizer):\n",
    "    \n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    \n",
    "    eva_n_gold, eva_n_pred,eva_n_tp = 0, 0, 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outs = model.generate(input_ids=batch['source_ids'].to(args.device), \n",
    "                                  attention_mask=batch['source_mask'].to(args.device), \n",
    "                                  max_length=args.max_seq_length)\n",
    "\n",
    "        pred_seqs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        gold_seqs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"target_ids\"]]\n",
    "\n",
    "        n_gold, n_pred, n_tp = compute_scores(pred_seqs, gold_seqs)\n",
    "        eva_n_gold += n_gold\n",
    "        eva_n_pred += n_pred\n",
    "        eva_n_tp += n_tp\n",
    "        \n",
    "    precision = float(eva_n_tp) / float(eva_n_pred) if eva_n_pred != 0 else 0\n",
    "    recall = float(eva_n_tp) / float(eva_n_gold) if eva_n_gold != 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision != 0 or recall != 0 else 0\n",
    "    f1 = torch.tensor(f1, dtype=torch.float32)\n",
    "    \n",
    "    print(f'n_gold:{eva_n_gold}; n_pred:{eva_n_pred}; n_tp:{eva_n_tp}')\n",
    "        \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a438a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(args, seed):\n",
    "    \n",
    "    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu' )\n",
    "\n",
    "    print(\"\\n\", \"=\" * 30, \"NEW train\", \"=\" * 30, \"\\n\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    # sanity check\n",
    "    # show one sample to check the code and the expected output\n",
    "    args.train = True\n",
    "    print(f\"Here is an example (from the train set):\")\n",
    "    train_dataset = ABSADataset(tokenizer=tokenizer, args = args,\n",
    "                                input_max_len=args.input_max_len, target_max_len=args.target_max_len)\n",
    "    data_sample = train_dataset[7]  # a random data sample\n",
    "    print('Input :', tokenizer.decode(data_sample['source_ids'], skip_special_tokens=True))\n",
    "    print('Output:', tokenizer.decode(data_sample['target_ids'], skip_special_tokens=True))\n",
    "    \n",
    "    args.train = False\n",
    "    print(f\"Here is an example (from the test set):\")\n",
    "    test_dataset = ABSADataset(tokenizer=tokenizer, args = args,\n",
    "                                input_max_len=args.input_max_len, target_max_len=args.target_max_len)\n",
    "    data_sample = test_dataset[7]  # a random data sample\n",
    "    print('Input :', tokenizer.decode(data_sample['source_ids'], skip_special_tokens=True))\n",
    "    print('Output:', tokenizer.decode(data_sample['target_ids'], skip_special_tokens=True))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=args.eval_batch_size, num_workers=4)\n",
    "\n",
    "    # initialize the T5 model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    # optimizer and lr scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_loader) * args.num_train_epochs),\n",
    "    )\n",
    "\n",
    "    # training process\n",
    "    print(\"start training......\")\n",
    "    best_Score = -10000\n",
    "    earlystop_count = 0\n",
    "\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            lm_labels = batch[\"target_ids\"]\n",
    "            lm_labels[lm_labels[:, :] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs = model(input_ids=batch[\"source_ids\"].to(args.device),\n",
    "                           attention_mask=batch[\"source_mask\"].to(args.device),\n",
    "                           labels=lm_labels.to(args.device),\n",
    "                           decoder_attention_mask=batch['target_mask'].to(args.device))\n",
    "\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.detach().float()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "        f1 = validation(args, model, val_loader, tokenizer)\n",
    "\n",
    "        current_Score = f1\n",
    "\n",
    "        if current_Score > best_Score:\n",
    "            print(f'bestF1:{f1}')\n",
    "            best_Score = current_Score\n",
    "            model_name  = f'main-multi-source-seed={seed}'\n",
    "            model_name = args.output_dir + '/' + model_name\n",
    "#                 tokenizer.save_pretrained(model_name)\n",
    "#                 model.save_pretrained(model_name)\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(\"Save current best model in file:\", model_name)\n",
    "            earlystop_count = 0\n",
    "        else:\n",
    "            earlystop_count+= 1\n",
    "            if earlystop_count == 10:\n",
    "#                     print(\"Load best model from:\", model_name)\n",
    "#                     test_f1 = test(args, model_name, test_loader, tokenizer)\n",
    "#                     print(f'Best Test F1:{test_f1}')\n",
    "                break\n",
    "\n",
    "        train_epoch_loss = total_loss / len(train_loader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        print(f\"epoch={epoch}: train_ppl:{train_ppl};train_loss:{train_epoch_loss};F1:{f1}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Finish training and saving the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12dd2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    seed_list = [1, 42, 1992, 2023, 2024]\n",
    "\n",
    "    for each_seed in seed_list:\n",
    "        # initialization\n",
    "        args.seed = each_seed\n",
    "        #order = \" \".join(each)\n",
    "\n",
    "        seed = args.seed  # random.randint(0, 1234)\n",
    "        print(\"seed \", seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "#         cudnn.benchmark = False\n",
    "#         torch.backends.cudnn.deterministic = True\n",
    "        train_function(args, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242a710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, load_path, dataloader, tokenizer):\n",
    "    \n",
    "    model = MyT5ForConditionalGeneration.from_pretrained(args.model_name_or_path)\n",
    "    \n",
    "    model.load_state_dict(torch.load(load_path)) \n",
    "    \n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    outputs, targets = [], []\n",
    "    \n",
    "    eva_n_gold, eva_n_pred,eva_n_tp = 0, 0, 0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        # need to push the data to device\n",
    "        with torch.no_grad():\n",
    "            outs = model.generate(input_ids=batch['source_ids'].to(args.device), \n",
    "                                  attention_mask=batch['source_mask'].to(args.device), \n",
    "                                  max_length=args.max_seq_length)\n",
    "\n",
    "        pred_seqs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        gold_seqs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"target_ids\"]]\n",
    "        outputs.extend(pred_seqs)\n",
    "        targets.extend(gold_seqs)\n",
    "\n",
    "        n_gold, n_pred, n_tp = compute_scores(pred_seqs, gold_seqs)\n",
    "        eva_n_gold += n_gold\n",
    "        eva_n_pred += n_pred\n",
    "        eva_n_tp += n_tp\n",
    "        \n",
    "    precision = float(eva_n_tp) / float(eva_n_pred) if eva_n_pred != 0 else 0\n",
    "    recall = float(eva_n_tp) / float(eva_n_gold) if eva_n_gold != 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision != 0 or recall != 0 else 0\n",
    "    f1 = torch.tensor(f1, dtype=torch.float32)\n",
    "    \n",
    "    print(f'n_gold:{eva_n_gold}; n_pred:{eva_n_pred}; n_tp:{eva_n_tp}')\n",
    "    print(Span_Evalution(outputs, targets).Quad_Evaluation())\n",
    "    \n",
    "    return f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
